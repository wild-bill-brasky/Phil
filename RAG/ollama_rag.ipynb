{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d60e71-b2fc-4a0b-b274-b7cb9ff452ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import utils as chromautils\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from torch import cuda\n",
    "from gc import collect\n",
    "\n",
    "import shutil\n",
    "\n",
    "#embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#embedding_model_name = \"ibm-granite/granite-embedding-125m-english\"\n",
    "embedding_model_name = 'BAAI/bge-large-en-v1.5'\n",
    "model = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fef35753",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect()\n",
    "cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a620bdf-6b7a-485b-a3b9-cfae13c01567",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files_directory = \"/home/spinnaker/py_dev/ai_training/rag_db/nissan\"\n",
    "\n",
    "def list_text_files(directory):\n",
    "    file_list = []\n",
    "    for (root, dirs, file) in os.walk(directory):\n",
    "        for f in file:\n",
    "            if '.txt' in f:\n",
    "                file_list.append(f'{root}/{f}')\n",
    "    return file_list\n",
    "\n",
    "# Specify the directory to search for PDF files\n",
    "txt_files = list_text_files(text_files_directory)\n",
    "print(txt_files)\n",
    "print(len(txt_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b411b6d-624c-4f47-af1e-8cea04517459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and embed the content of the log files\n",
    "def load_and_embed_files(file_paths):\n",
    "    documents = []\n",
    "    for file_path in file_paths:\n",
    "        loader = UnstructuredLoader(file_path, mode=\"elements\")\n",
    "        documents.extend(loader.load_and_split())\n",
    "        documents = chromautils.filter_complex_metadata(documents)\n",
    "    return documents\n",
    "documents = load_and_embed_files(txt_files)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70099758",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.clear()\n",
    "txt_files.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40234e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents list into sublists to be fed in a loop for vectorization.\n",
    "def split_into_chunks(lst, chunk_size):\n",
    "    chunks = []\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        chunks.append(lst[i:i + chunk_size])\n",
    "    return chunks\n",
    "documents_list = split_into_chunks(documents, 40000)\n",
    "print(len(documents_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028eb79-0b6d-48ea-b941-e885495b6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vector store from the documents / logs you provided\n",
    "# Max batch size for embedding is 41666\n",
    "v_path_vector_store = '/home/spinnaker/py_dev/ai_training/rag_db/nissan_db'\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embedding_model, persist_directory=v_path_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48928fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to existing vector store\n",
    "for i in documents_list:\n",
    "    v_path_vector_store = '/home/spinnaker/py_dev/ai_training/rag_db/cyber_db'\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    vector_store = Chroma(embedding_function=embedding_model, persist_directory=v_path_vector_store)\n",
    "    vector_store.add_documents(documents=i)\n",
    "    print('Vector store done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ec4682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Properly delete old vector store to reuse same disk space\n",
    "v_path_vector_store = '/home/spinnaker/py_dev/ai_training/rag_db/nissan_db'\n",
    "shutil.rmtree(v_path_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c6cc1-cc60-4ffc-b30c-6f4450f2db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vectorstore from disk\n",
    "v_path_vector_store = '/home/spinnaker/py_dev/ai_training/rag_db/nissan_db'\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "chroma_db = Chroma(persist_directory=v_path_vector_store, embedding_function=embedding_model)\n",
    "type(chroma_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2053f1-ef5a-4f1c-bf7e-d1b22217cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_model = Ollama(model=model, verbose=False)  # Disable verbose for batch processing\n",
    "llm = ChatOllama(model=model, temperature=0, num_ctx=4096, repeat_last_n=10000, verbose=False, keep_alive=0)\n",
    "print(f\"Loaded LLM model {llm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849df4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = chroma_db.as_retriever(search_kwargs={f\"k\": 20})  # Use the number of documents to retrieve\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=retriever,)\n",
    "\n",
    "# Use the 'invoke' method to handle the query\n",
    "result = qa_chain.invoke({\"query\": 'You have been loaded with the 2022 Nissan Frontier Owners Manual in a RAG database. Using this information, write a summary of anything mentioned about maintenance schedules.'})\n",
    "print(result.get('result'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-training",
   "language": "python",
   "name": "ai-training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
